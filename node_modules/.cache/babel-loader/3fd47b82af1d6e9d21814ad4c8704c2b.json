{"ast":null,"code":"import _slicedToArray from\"F:/Project-Legal.ly/node_modules/@babel/runtime/helpers/esm/slicedToArray.js\";import React,{useState,useEffect}from\"react\";import SpeechRecognition,{useSpeechRecognition}from\"react-speech-recognition\";import Mic from\"../Assets/mic.svg\";import OnMic from\"../Assets/micon.svg\";import{jsx as _jsx}from\"react/jsx-runtime\";var Dictaphone=function Dictaphone(_ref){var input=_ref.input,setinput=_ref.setinput;var _useState=useState(false),_useState2=_slicedToArray(_useState,2),mic=_useState2[0],setMic=_useState2[1];var _useState3=useState(\"en-IN\"),_useState4=_slicedToArray(_useState3,2),language=_useState4[0],setLanguage=_useState4[1];// Initial language\nvar _useSpeechRecognition=useSpeechRecognition(),transcript=_useSpeechRecognition.transcript,listening=_useSpeechRecognition.listening,resetTranscript=_useSpeechRecognition.resetTranscript,browserSupportsSpeechRecognition=_useSpeechRecognition.browserSupportsSpeechRecognition;useEffect(function(){// Check for language change in transcript\nif(transcript&&transcript.toLowerCase().includes(\"tamizh\")){setLanguage(\"ta-IN\");}else if(transcript&&transcript.toLowerCase().includes(\"english\")){setLanguage(\"en-IN\");}},[transcript]);if(!browserSupportsSpeechRecognition){return alert(\"Browser doesn't support speech recognition\");}var onMicStart=function onMicStart(){setMic(!mic);if(mic){SpeechRecognition.startListening({language:language// Use the dynamically determined language\n});}else{SpeechRecognition.stopListening();}console.log(transcript);setinput(transcript);};return/*#__PURE__*/_jsx(\"div\",{children:/*#__PURE__*/_jsx(\"img\",{src:listening?OnMic:Mic,alt:\"Mic\",className:\"w-[4.5vw] h-[5vh] mt-2 px-0 cursor-pointer transition-all user-select-none\",onClick:onMicStart})});};export default Dictaphone;// import React, { useState } from \"react\";\n// import SpeechRecognition, {\n//   useSpeechRecognition,\n// } from \"react-speech-recognition\";\n// import Mic from \"../Assets/mic.svg\";\n// import OnMic from \"../Assets/micon.svg\";\n// const Dictaphone = ({ input, setinput }) => {\n//   const [mic, setMic] = useState(false);\n//   const {\n//     transcript,\n//     listening,\n//     resetTranscript,\n//     browserSupportsSpeechRecognition,\n//   } = useSpeechRecognition();\n//   if (!browserSupportsSpeechRecognition) {\n//     return alert(\"Browser doesn't support speech recognition\");\n//   }\n//   const onMicStart = () => {\n//     setMic(!mic);\n//     mic\n//       ? SpeechRecognition.startListening()\n//       : SpeechRecognition.stopListening();\n//     console.log(transcript);\n//     setinput(transcript);\n//   };\n//   return (\n//     <div>\n//       <img\n//         src={listening ? OnMic : Mic}\n//         alt=\"Mic\"\n//         className=\"w-[4.5vw] h-[5vh] mt-2 px-0 cursor-pointer transition-all user-select-none\"\n//         onClick={onMicStart}\n//       />\n//     </div>\n//   );\n// };\n// export default Dictaphone;","map":{"version":3,"sources":["F:/Project-Legal.ly/src/Components/Dictaphone.js"],"names":["React","useState","useEffect","SpeechRecognition","useSpeechRecognition","Mic","OnMic","Dictaphone","input","setinput","mic","setMic","language","setLanguage","transcript","listening","resetTranscript","browserSupportsSpeechRecognition","toLowerCase","includes","alert","onMicStart","startListening","stopListening","console","log"],"mappings":"yGAAA,MAAOA,CAAAA,KAAP,EAAgBC,QAAhB,CAA0BC,SAA1B,KAA2C,OAA3C,CACA,MAAOC,CAAAA,iBAAP,EAA4BC,oBAA5B,KAAwD,0BAAxD,CACA,MAAOC,CAAAA,GAAP,KAAgB,mBAAhB,CACA,MAAOC,CAAAA,KAAP,KAAkB,qBAAlB,C,2CAEA,GAAMC,CAAAA,UAAU,CAAG,QAAbA,CAAAA,UAAa,MAAyB,IAAtBC,CAAAA,KAAsB,MAAtBA,KAAsB,CAAfC,QAAe,MAAfA,QAAe,CAC1C,cAAsBR,QAAQ,CAAC,KAAD,CAA9B,wCAAOS,GAAP,eAAYC,MAAZ,eACA,eAAgCV,QAAQ,CAAC,OAAD,CAAxC,yCAAOW,QAAP,eAAiBC,WAAjB,eAAmD;AACnD,0BAKIT,oBAAoB,EALxB,CACEU,UADF,uBACEA,UADF,CAEEC,SAFF,uBAEEA,SAFF,CAGEC,eAHF,uBAGEA,eAHF,CAIEC,gCAJF,uBAIEA,gCAJF,CAOAf,SAAS,CAAC,UAAM,CACd;AACA,GAAIY,UAAU,EAAIA,UAAU,CAACI,WAAX,GAAyBC,QAAzB,CAAkC,QAAlC,CAAlB,CAA+D,CAC7DN,WAAW,CAAC,OAAD,CAAX,CACD,CAFD,IAEO,IAAIC,UAAU,EAAIA,UAAU,CAACI,WAAX,GAAyBC,QAAzB,CAAkC,SAAlC,CAAlB,CAAgE,CACrEN,WAAW,CAAC,OAAD,CAAX,CACD,CACF,CAPQ,CAON,CAACC,UAAD,CAPM,CAAT,CASA,GAAI,CAACG,gCAAL,CAAuC,CACrC,MAAOG,CAAAA,KAAK,CAAC,4CAAD,CAAZ,CACD,CAED,GAAMC,CAAAA,UAAU,CAAG,QAAbA,CAAAA,UAAa,EAAM,CACvBV,MAAM,CAAC,CAACD,GAAF,CAAN,CACA,GAAIA,GAAJ,CAAS,CACPP,iBAAiB,CAACmB,cAAlB,CAAiC,CAC/BV,QAAQ,CAAEA,QAAU;AADW,CAAjC,EAGD,CAJD,IAIO,CACLT,iBAAiB,CAACoB,aAAlB,GACD,CACDC,OAAO,CAACC,GAAR,CAAYX,UAAZ,EACAL,QAAQ,CAACK,UAAD,CAAR,CACD,CAXD,CAaA,mBACE,kCACE,YACE,GAAG,CAAEC,SAAS,CAAGT,KAAH,CAAWD,GAD3B,CAEE,GAAG,CAAC,KAFN,CAGE,SAAS,CAAC,4EAHZ,CAIE,OAAO,CAAEgB,UAJX,EADF,EADF,CAUD,CA9CD,CAgDA,cAAed,CAAAA,UAAf,CACA;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","sourcesContent":["import React, { useState, useEffect } from \"react\";\r\nimport SpeechRecognition, { useSpeechRecognition } from \"react-speech-recognition\";\r\nimport Mic from \"../Assets/mic.svg\";\r\nimport OnMic from \"../Assets/micon.svg\";\r\n\r\nconst Dictaphone = ({ input, setinput }) => {\r\n  const [mic, setMic] = useState(false);\r\n  const [language, setLanguage] = useState(\"en-IN\"); // Initial language\r\n  const {\r\n    transcript,\r\n    listening,\r\n    resetTranscript,\r\n    browserSupportsSpeechRecognition,\r\n  } = useSpeechRecognition();\r\n\r\n  useEffect(() => {\r\n    // Check for language change in transcript\r\n    if (transcript && transcript.toLowerCase().includes(\"tamizh\")) {\r\n      setLanguage(\"ta-IN\");\r\n    } else if (transcript && transcript.toLowerCase().includes(\"english\")) {\r\n      setLanguage(\"en-IN\");\r\n    }\r\n  }, [transcript]);\r\n\r\n  if (!browserSupportsSpeechRecognition) {\r\n    return alert(\"Browser doesn't support speech recognition\");\r\n  }\r\n\r\n  const onMicStart = () => {\r\n    setMic(!mic);\r\n    if (mic) {\r\n      SpeechRecognition.startListening({\r\n        language: language, // Use the dynamically determined language\r\n      });\r\n    } else {\r\n      SpeechRecognition.stopListening();\r\n    }\r\n    console.log(transcript);\r\n    setinput(transcript);\r\n  };\r\n\r\n  return (\r\n    <div>\r\n      <img\r\n        src={listening ? OnMic : Mic}\r\n        alt=\"Mic\"\r\n        className=\"w-[4.5vw] h-[5vh] mt-2 px-0 cursor-pointer transition-all user-select-none\"\r\n        onClick={onMicStart}\r\n      />\r\n    </div>\r\n  );\r\n};\r\n\r\nexport default Dictaphone;\r\n// import React, { useState } from \"react\";\r\n\r\n// import SpeechRecognition, {\r\n//   useSpeechRecognition,\r\n// } from \"react-speech-recognition\";\r\n// import Mic from \"../Assets/mic.svg\";\r\n// import OnMic from \"../Assets/micon.svg\";\r\n\r\n// const Dictaphone = ({ input, setinput }) => {\r\n//   const [mic, setMic] = useState(false);\r\n//   const {\r\n//     transcript,\r\n//     listening,\r\n//     resetTranscript,\r\n//     browserSupportsSpeechRecognition,\r\n//   } = useSpeechRecognition();\r\n\r\n//   if (!browserSupportsSpeechRecognition) {\r\n//     return alert(\"Browser doesn't support speech recognition\");\r\n//   }\r\n//   const onMicStart = () => {\r\n//     setMic(!mic);\r\n//     mic\r\n//       ? SpeechRecognition.startListening()\r\n//       : SpeechRecognition.stopListening();\r\n//     console.log(transcript);\r\n//     setinput(transcript);\r\n//   };\r\n\r\n//   return (\r\n//     <div>\r\n//       <img\r\n//         src={listening ? OnMic : Mic}\r\n//         alt=\"Mic\"\r\n//         className=\"w-[4.5vw] h-[5vh] mt-2 px-0 cursor-pointer transition-all user-select-none\"\r\n//         onClick={onMicStart}\r\n//       />\r\n//     </div>\r\n//   );\r\n// };\r\n// export default Dictaphone;\r\n"]},"metadata":{},"sourceType":"module"}