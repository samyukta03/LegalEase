{"ast":null,"code":"import _slicedToArray from\"F:/Project-Legal.ly/node_modules/@babel/runtime/helpers/esm/slicedToArray.js\";import React,{useState}from\"react\";import SpeechRecognition,{useSpeechRecognition}from\"react-speech-recognition\";import{franc}from'franc';import{jsx as _jsx}from\"react/jsx-runtime\";import{jsxs as _jsxs}from\"react/jsx-runtime\";var Dictaphone=function Dictaphone(_ref){var input=_ref.input,setinput=_ref.setinput;var _useState=useState(false),_useState2=_slicedToArray(_useState,2),mic=_useState2[0],setMic=_useState2[1];var _useSpeechRecognition=useSpeechRecognition(),transcript=_useSpeechRecognition.transcript,listening=_useSpeechRecognition.listening,resetTranscript=_useSpeechRecognition.resetTranscript,browserSupportsSpeechRecognition=_useSpeechRecognition.browserSupportsSpeechRecognition;if(!browserSupportsSpeechRecognition){return alert(\"Browser doesn't support speech recognition\");}var onMicStart=function onMicStart(){setMic(!mic);mic?SpeechRecognition.startListening({continuous:true,language:\"en-US\"}):SpeechRecognition.stopListening();};var handleTranscript=function handleTranscript(newTranscript){setinput(newTranscript);handleLanguageSwitch(newTranscript);};var handleLanguageSwitch=function handleLanguageSwitch(text){var detectedLang=franc(text);var lang=detectedLang===\"eng\"?\"en-US\":\"ta-IN\";SpeechRecognition.abortListening();SpeechRecognition.startListening({continuous:true,language:lang});};return/*#__PURE__*/_jsxs(\"div\",{children:[/*#__PURE__*/_jsx(\"button\",{onClick:resetTranscript,children:\"Reset\"}),/*#__PURE__*/_jsx(\"button\",{onClick:onMicStart,children:listening?\"Stop\":\"Start\"}),/*#__PURE__*/_jsx(\"p\",{children:transcript}),/*#__PURE__*/_jsxs(\"p\",{children:[\"Detected Language: \",franc(transcript)]})]});};export default Dictaphone;// import React, { useState } from \"react\";\n// import SpeechRecognition, {\n//   useSpeechRecognition,\n// } from \"react-speech-recognition\";\n// import Mic from \"../Assets/mic.svg\";\n// import OnMic from \"../Assets/micon.svg\";\n// const Dictaphone = ({ input, setinput }) => {\n//   const [mic, setMic] = useState(false);\n//   const {\n//     transcript,\n//     listening,\n//     resetTranscript,\n//     browserSupportsSpeechRecognition,\n//   } = useSpeechRecognition();\n//   if (!browserSupportsSpeechRecognition) {\n//     return alert(\"Browser doesn't support speech recognition\");\n//   }\n//   const onMicStart = () => {\n//     setMic(!mic);\n//     mic\n//       ? SpeechRecognition.startListening({ language: \"en,ta-IN\" })\n//       : SpeechRecognition.stopListening();\n//     console.log(transcript);\n//     setinput(transcript);\n//   };\n//   return (\n//     <div>\n//       <img\n//         src={listening ? OnMic : Mic}\n//         alt=\"Mic\"\n//         className=\"w-[4.5vw] h-[5vh] mt-2 px-0 cursor-pointer transition-all user-select-none\"\n//         onClick={onMicStart}\n//       />\n//     </div>\n//   );\n// };\n// export default Dictaphone;","map":{"version":3,"sources":["F:/Project-Legal.ly/src/Components/Dictaphone.js"],"names":["React","useState","SpeechRecognition","useSpeechRecognition","franc","Dictaphone","input","setinput","mic","setMic","transcript","listening","resetTranscript","browserSupportsSpeechRecognition","alert","onMicStart","startListening","continuous","language","stopListening","handleTranscript","newTranscript","handleLanguageSwitch","text","detectedLang","lang","abortListening"],"mappings":"yGAAA,MAAOA,CAAAA,KAAP,EAAgBC,QAAhB,KAAgC,OAAhC,CACA,MAAOC,CAAAA,iBAAP,EAA4BC,oBAA5B,KAAwD,0BAAxD,CACA,OAASC,KAAT,KAAsB,OAAtB,C,wFAEA,GAAMC,CAAAA,UAAU,CAAG,QAAbA,CAAAA,UAAa,MAAyB,IAAtBC,CAAAA,KAAsB,MAAtBA,KAAsB,CAAfC,QAAe,MAAfA,QAAe,CAC1C,cAAsBN,QAAQ,CAAC,KAAD,CAA9B,wCAAOO,GAAP,eAAYC,MAAZ,eACA,0BAKIN,oBAAoB,EALxB,CACEO,UADF,uBACEA,UADF,CAEEC,SAFF,uBAEEA,SAFF,CAGEC,eAHF,uBAGEA,eAHF,CAIEC,gCAJF,uBAIEA,gCAJF,CAOA,GAAI,CAACA,gCAAL,CAAuC,CACrC,MAAOC,CAAAA,KAAK,CAAC,4CAAD,CAAZ,CACD,CAED,GAAMC,CAAAA,UAAU,CAAG,QAAbA,CAAAA,UAAa,EAAM,CACvBN,MAAM,CAAC,CAACD,GAAF,CAAN,CACAA,GAAG,CACCN,iBAAiB,CAACc,cAAlB,CAAiC,CAAEC,UAAU,CAAE,IAAd,CAAoBC,QAAQ,CAAE,OAA9B,CAAjC,CADD,CAEChB,iBAAiB,CAACiB,aAAlB,EAFJ,CAGD,CALD,CAOA,GAAMC,CAAAA,gBAAgB,CAAG,QAAnBA,CAAAA,gBAAmB,CAACC,aAAD,CAAmB,CAC1Cd,QAAQ,CAACc,aAAD,CAAR,CACAC,oBAAoB,CAACD,aAAD,CAApB,CACD,CAHD,CAKA,GAAMC,CAAAA,oBAAoB,CAAG,QAAvBA,CAAAA,oBAAuB,CAACC,IAAD,CAAU,CACrC,GAAMC,CAAAA,YAAY,CAAGpB,KAAK,CAACmB,IAAD,CAA1B,CACA,GAAME,CAAAA,IAAI,CAAGD,YAAY,GAAK,KAAjB,CAAyB,OAAzB,CAAmC,OAAhD,CACAtB,iBAAiB,CAACwB,cAAlB,GACAxB,iBAAiB,CAACc,cAAlB,CAAiC,CAAEC,UAAU,CAAE,IAAd,CAAoBC,QAAQ,CAAEO,IAA9B,CAAjC,EACD,CALD,CAOA,mBACE,oCACE,eAAQ,OAAO,CAAEb,eAAjB,mBADF,cAEE,eAAQ,OAAO,CAAEG,UAAjB,UAA8BJ,SAAS,CAAG,MAAH,CAAY,OAAnD,EAFF,cAGE,mBAAID,UAAJ,EAHF,cAIE,2CAAuBN,KAAK,CAACM,UAAD,CAA5B,GAJF,GADF,CAQD,CAxCD,CAyCA,cAAeL,CAAAA,UAAf,CACA;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","sourcesContent":["import React, { useState } from \"react\";\r\nimport SpeechRecognition, { useSpeechRecognition } from \"react-speech-recognition\";\r\nimport { franc } from 'franc';\r\n\r\nconst Dictaphone = ({ input, setinput }) => {\r\n  const [mic, setMic] = useState(false);\r\n  const {\r\n    transcript,\r\n    listening,\r\n    resetTranscript,\r\n    browserSupportsSpeechRecognition,\r\n  } = useSpeechRecognition();\r\n\r\n  if (!browserSupportsSpeechRecognition) {\r\n    return alert(\"Browser doesn't support speech recognition\");\r\n  }\r\n\r\n  const onMicStart = () => {\r\n    setMic(!mic);\r\n    mic\r\n      ? SpeechRecognition.startListening({ continuous: true, language: \"en-US\" })\r\n      : SpeechRecognition.stopListening();\r\n  };\r\n\r\n  const handleTranscript = (newTranscript) => {\r\n    setinput(newTranscript);\r\n    handleLanguageSwitch(newTranscript);\r\n  };\r\n\r\n  const handleLanguageSwitch = (text) => {\r\n    const detectedLang = franc(text);\r\n    const lang = detectedLang === \"eng\" ? \"en-US\" : \"ta-IN\";\r\n    SpeechRecognition.abortListening();\r\n    SpeechRecognition.startListening({ continuous: true, language: lang });\r\n  };\r\n\r\n  return (\r\n    <div>\r\n      <button onClick={resetTranscript}>Reset</button>\r\n      <button onClick={onMicStart}>{listening ? \"Stop\" : \"Start\"}</button>\r\n      <p>{transcript}</p>\r\n      <p>Detected Language: {franc(transcript)}</p>\r\n    </div>\r\n  );\r\n};\r\nexport default Dictaphone;\r\n// import React, { useState } from \"react\";\r\n\r\n// import SpeechRecognition, {\r\n//   useSpeechRecognition,\r\n// } from \"react-speech-recognition\";\r\n// import Mic from \"../Assets/mic.svg\";\r\n// import OnMic from \"../Assets/micon.svg\";\r\n\r\n// const Dictaphone = ({ input, setinput }) => {\r\n//   const [mic, setMic] = useState(false);\r\n//   const {\r\n//     transcript,\r\n//     listening,\r\n//     resetTranscript,\r\n//     browserSupportsSpeechRecognition,\r\n//   } = useSpeechRecognition();\r\n\r\n//   if (!browserSupportsSpeechRecognition) {\r\n//     return alert(\"Browser doesn't support speech recognition\");\r\n//   }\r\n//   const onMicStart = () => {\r\n//     setMic(!mic);\r\n//     mic\r\n//       ? SpeechRecognition.startListening({ language: \"en,ta-IN\" })\r\n//       : SpeechRecognition.stopListening();\r\n//     console.log(transcript);\r\n//     setinput(transcript);\r\n//   };\r\n\r\n//   return (\r\n//     <div>\r\n//       <img\r\n//         src={listening ? OnMic : Mic}\r\n//         alt=\"Mic\"\r\n//         className=\"w-[4.5vw] h-[5vh] mt-2 px-0 cursor-pointer transition-all user-select-none\"\r\n//         onClick={onMicStart}\r\n//       />\r\n//     </div>\r\n//   );\r\n// };\r\n// export default Dictaphone;\r\n"]},"metadata":{},"sourceType":"module"}