{"ast":null,"code":"import _slicedToArray from\"F:/Project-Legal.ly/node_modules/@babel/runtime/helpers/esm/slicedToArray.js\";import React,{useState}from\"react\";import SpeechRecognition,{useSpeechRecognition}from\"react-speech-recognition\";import language from\"detect-language\";import Mic from\"../Assets/mic.svg\";import OnMic from\"../Assets/micon.svg\";import{jsx as _jsx}from\"react/jsx-runtime\";import{jsxs as _jsxs}from\"react/jsx-runtime\";var Dictaphone=function Dictaphone(_ref){var input=_ref.input,setinput=_ref.setinput;var _useState=useState(false),_useState2=_slicedToArray(_useState,2),mic=_useState2[0],setMic=_useState2[1];var _useSpeechRecognition=useSpeechRecognition(),transcript=_useSpeechRecognition.transcript,listening=_useSpeechRecognition.listening,resetTranscript=_useSpeechRecognition.resetTranscript,browserSupportsSpeechRecognition=_useSpeechRecognition.browserSupportsSpeechRecognition;if(!browserSupportsSpeechRecognition){return alert(\"Browser doesn't support speech recognition\");}var onMicStart=function onMicStart(){setMic(!mic);mic?SpeechRecognition.startListening():SpeechRecognition.stopListening();};var handleLanguageDetection=function handleLanguageDetection(){language.detect(transcript,function(result){console.log(\"Detected Language:\",result.language);});};var handleSpeechRecognition=function handleSpeechRecognition(){if(transcript){handleLanguageDetection();setinput(transcript);resetTranscript();}};return/*#__PURE__*/_jsxs(\"div\",{children:[/*#__PURE__*/_jsx(\"img\",{src:listening?OnMic:Mic,alt:\"Mic\",className:\"w-[4.5vw] h-[5vh] mt-2 px-0 cursor-pointer transition-all user-select-none\",onClick:onMicStart}),/*#__PURE__*/_jsx(\"button\",{onClick:handleSpeechRecognition,children:\"Recognize Speech\"}),/*#__PURE__*/_jsxs(\"p\",{children:[\"Transcript: \",transcript]})]});};export default Dictaphone;// import React, { useState } from \"react\";\n// import SpeechRecognition, {\n//   useSpeechRecognition,\n// } from \"react-speech-recognition\";\n// import Mic from \"../Assets/mic.svg\";\n// import OnMic from \"../Assets/micon.svg\";\n// const Dictaphone = ({ input, setinput }) => {\n//   const [mic, setMic] = useState(false);\n//   const {\n//     transcript,\n//     listening,\n//     resetTranscript,\n//     browserSupportsSpeechRecognition,\n//   } = useSpeechRecognition();\n//   if (!browserSupportsSpeechRecognition) {\n//     return alert(\"Browser doesn't support speech recognition\");\n//   }\n//   const onMicStart = () => {\n//     setMic(!mic);\n//     mic\n//       ? SpeechRecognition.startListening({ language: \"en,ta-IN\" })\n//       : SpeechRecognition.stopListening();\n//     console.log(transcript);\n//     setinput(transcript);\n//   };\n//   return (\n//     <div>\n//       <img\n//         src={listening ? OnMic : Mic}\n//         alt=\"Mic\"\n//         className=\"w-[4.5vw] h-[5vh] mt-2 px-0 cursor-pointer transition-all user-select-none\"\n//         onClick={onMicStart}\n//       />\n//     </div>\n//   );\n// };\n// export default Dictaphone;","map":{"version":3,"sources":["F:/Project-Legal.ly/src/Components/Dictaphone.js"],"names":["React","useState","SpeechRecognition","useSpeechRecognition","language","Mic","OnMic","Dictaphone","input","setinput","mic","setMic","transcript","listening","resetTranscript","browserSupportsSpeechRecognition","alert","onMicStart","startListening","stopListening","handleLanguageDetection","detect","result","console","log","handleSpeechRecognition"],"mappings":"yGAAA,MAAOA,CAAAA,KAAP,EAAgBC,QAAhB,KAAgC,OAAhC,CACA,MAAOC,CAAAA,iBAAP,EAA4BC,oBAA5B,KAAwD,0BAAxD,CACA,MAAOC,CAAAA,QAAP,KAAqB,iBAArB,CACA,MAAOC,CAAAA,GAAP,KAAgB,mBAAhB,CACA,MAAOC,CAAAA,KAAP,KAAkB,qBAAlB,C,wFAEA,GAAMC,CAAAA,UAAU,CAAG,QAAbA,CAAAA,UAAa,MAAyB,IAAtBC,CAAAA,KAAsB,MAAtBA,KAAsB,CAAfC,QAAe,MAAfA,QAAe,CAC1C,cAAsBR,QAAQ,CAAC,KAAD,CAA9B,wCAAOS,GAAP,eAAYC,MAAZ,eACA,0BAKIR,oBAAoB,EALxB,CACES,UADF,uBACEA,UADF,CAEEC,SAFF,uBAEEA,SAFF,CAGEC,eAHF,uBAGEA,eAHF,CAIEC,gCAJF,uBAIEA,gCAJF,CAOA,GAAI,CAACA,gCAAL,CAAuC,CACrC,MAAOC,CAAAA,KAAK,CAAC,4CAAD,CAAZ,CACD,CAED,GAAMC,CAAAA,UAAU,CAAG,QAAbA,CAAAA,UAAa,EAAM,CACvBN,MAAM,CAAC,CAACD,GAAF,CAAN,CACAA,GAAG,CACCR,iBAAiB,CAACgB,cAAlB,EADD,CAEChB,iBAAiB,CAACiB,aAAlB,EAFJ,CAGD,CALD,CAOA,GAAMC,CAAAA,uBAAuB,CAAG,QAA1BA,CAAAA,uBAA0B,EAAM,CACpChB,QAAQ,CAACiB,MAAT,CAAgBT,UAAhB,CAA4B,SAACU,MAAD,CAAY,CACtCC,OAAO,CAACC,GAAR,CAAY,oBAAZ,CAAkCF,MAAM,CAAClB,QAAzC,EACD,CAFD,EAGD,CAJD,CAMA,GAAMqB,CAAAA,uBAAuB,CAAG,QAA1BA,CAAAA,uBAA0B,EAAM,CACpC,GAAIb,UAAJ,CAAgB,CACdQ,uBAAuB,GACvBX,QAAQ,CAACG,UAAD,CAAR,CACAE,eAAe,GAChB,CACF,CAND,CAQA,mBACE,oCACE,YACE,GAAG,CAAED,SAAS,CAAGP,KAAH,CAAWD,GAD3B,CAEE,GAAG,CAAC,KAFN,CAGE,SAAS,CAAC,4EAHZ,CAIE,OAAO,CAAEY,UAJX,EADF,cAOE,eAAQ,OAAO,CAAEQ,uBAAjB,8BAPF,cAQE,oCAAgBb,UAAhB,GARF,GADF,CAYD,CA9CD,CAgDA,cAAeL,CAAAA,UAAf,CACA;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","sourcesContent":["import React, { useState } from \"react\";\r\nimport SpeechRecognition, { useSpeechRecognition } from \"react-speech-recognition\";\r\nimport language from \"detect-language\";\r\nimport Mic from \"../Assets/mic.svg\";\r\nimport OnMic from \"../Assets/micon.svg\";\r\n\r\nconst Dictaphone = ({ input, setinput }) => {\r\n  const [mic, setMic] = useState(false);\r\n  const {\r\n    transcript,\r\n    listening,\r\n    resetTranscript,\r\n    browserSupportsSpeechRecognition,\r\n  } = useSpeechRecognition();\r\n\r\n  if (!browserSupportsSpeechRecognition) {\r\n    return alert(\"Browser doesn't support speech recognition\");\r\n  }\r\n\r\n  const onMicStart = () => {\r\n    setMic(!mic);\r\n    mic\r\n      ? SpeechRecognition.startListening()\r\n      : SpeechRecognition.stopListening();\r\n  };\r\n\r\n  const handleLanguageDetection = () => {\r\n    language.detect(transcript, (result) => {\r\n      console.log(\"Detected Language:\", result.language);\r\n    });\r\n  };\r\n\r\n  const handleSpeechRecognition = () => {\r\n    if (transcript) {\r\n      handleLanguageDetection();\r\n      setinput(transcript);\r\n      resetTranscript();\r\n    }\r\n  };\r\n\r\n  return (\r\n    <div>\r\n      <img\r\n        src={listening ? OnMic : Mic}\r\n        alt=\"Mic\"\r\n        className=\"w-[4.5vw] h-[5vh] mt-2 px-0 cursor-pointer transition-all user-select-none\"\r\n        onClick={onMicStart}\r\n      />\r\n      <button onClick={handleSpeechRecognition}>Recognize Speech</button>\r\n      <p>Transcript: {transcript}</p>\r\n    </div>\r\n  );\r\n};\r\n\r\nexport default Dictaphone;\r\n// import React, { useState } from \"react\";\r\n\r\n// import SpeechRecognition, {\r\n//   useSpeechRecognition,\r\n// } from \"react-speech-recognition\";\r\n// import Mic from \"../Assets/mic.svg\";\r\n// import OnMic from \"../Assets/micon.svg\";\r\n\r\n// const Dictaphone = ({ input, setinput }) => {\r\n//   const [mic, setMic] = useState(false);\r\n//   const {\r\n//     transcript,\r\n//     listening,\r\n//     resetTranscript,\r\n//     browserSupportsSpeechRecognition,\r\n//   } = useSpeechRecognition();\r\n\r\n//   if (!browserSupportsSpeechRecognition) {\r\n//     return alert(\"Browser doesn't support speech recognition\");\r\n//   }\r\n//   const onMicStart = () => {\r\n//     setMic(!mic);\r\n//     mic\r\n//       ? SpeechRecognition.startListening({ language: \"en,ta-IN\" })\r\n//       : SpeechRecognition.stopListening();\r\n//     console.log(transcript);\r\n//     setinput(transcript);\r\n//   };\r\n\r\n//   return (\r\n//     <div>\r\n//       <img\r\n//         src={listening ? OnMic : Mic}\r\n//         alt=\"Mic\"\r\n//         className=\"w-[4.5vw] h-[5vh] mt-2 px-0 cursor-pointer transition-all user-select-none\"\r\n//         onClick={onMicStart}\r\n//       />\r\n//     </div>\r\n//   );\r\n// };\r\n// export default Dictaphone;\r\n"]},"metadata":{},"sourceType":"module"}